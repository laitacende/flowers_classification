{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNPgHZ6GyrhD/USU/lQDuh/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T4WWM2TCP26q","executionInfo":{"status":"ok","timestamp":1701505854709,"user_tz":-60,"elapsed":9194,"user":{"displayName":"Ola Gołębiowska","userId":"06307342828668766183"}},"outputId":"00b4a6f3-2600-4a74-cae5-f0ea291f5abc"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["from sklearn.datasets import fetch_20newsgroups\n","import keras\n","from keras import models\n","from keras import layers\n","from keras.preprocessing.text import Tokenizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, Flatten, TextVectorization, InputLayer, Input, Dropout\n","from keras.layers import Embedding\n","from keras.utils import to_categorical\n","import tensorflow as tf\n","import numpy as np\n","from nltk.stem.lancaster import LancasterStemmer\n","from sklearn.model_selection import train_test_split\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","tf.config.run_functions_eagerly(True)"]},{"cell_type":"code","source":["# all = fetch_20newsgroups(subset='all', shuffle=True, random_state=42)\n","train = fetch_20newsgroups(subset='train', shuffle=True, random_state = 42)\n","test = fetch_20newsgroups(subset='test', shuffle=True, random_state = 42)\n","unique, counts = np.unique(train.target, return_counts=True)\n","print(counts)\n","# print(train.data[:10])\n","# print(train.target[:10])"],"metadata":{"id":"SYAJNzz6Q1wH","executionInfo":{"status":"ok","timestamp":1701505872185,"user_tz":-60,"elapsed":17477,"user":{"displayName":"Ola Gołębiowska","userId":"06307342828668766183"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bd6a9754-9fed-4619-b01c-38d50d15ed8a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[480 584 591 590 578 593 585 594 598 597 600 595 591 594 593 599 546 564\n"," 465 377]\n"]}]},{"cell_type":"code","source":["MAX_SEQUENCE_LENGTH = 1000\n","MAX_WORDS = 20000\n","\n","def remove_stop_words_and_stem(text):\n","    text = word_tokenize(text.lower())\n","    stemmer = LancasterStemmer()\n","    stop_words = set(stopwords.words('english'))\n","    text = [stemmer.stem(word.lower()) for word in text]\n","    text = [word for word in text if word.isalpha() and not word in stop_words]\n","    return ' '.join(text)\n","\n","# train.data = list(map(remove_stop_words_and_stem, train.data))\n","# test.data = list(map(remove_stop_words_and_stem, test.data))\n","\n","# X_train = list(map(remove_stop_words_and_stem, X_train))\n","# X_test = list(map(remove_stop_words_and_stem, X_test))\n","\n","# vectorize_layer = TextVectorization(\n","#  max_tokens=MAX_NB_WORDS,\n","#  output_mode='int',\n","#  output_sequence_length=MAX_SEQUENCE_LENGTH)\n","# vectorize_layer.adapt(train.data)\n","\n","tokenizer = Tokenizer(num_words=MAX_WORDS)\n","tokenizer.fit_on_texts(train.data)\n","sequences = tokenizer.texts_to_sequences(train.data)\n","\n","word_index = tokenizer.word_index\n","\n","print('Found %s unique tokens.' % len(word_index))\n","\n","train_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","train_target = to_categorical(np.asarray(train.target))\n","print('Shape of data tensor:', train_data.shape)\n","# print('Shape of label tensor:', train_target.shape)\n","\n","sequences1 = tokenizer.texts_to_sequences(test.data)\n","test_data = pad_sequences(sequences1, maxlen=MAX_SEQUENCE_LENGTH)\n","test_target = to_categorical(np.asarray(test.target))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-1ntEjDVRV6d","executionInfo":{"status":"ok","timestamp":1701513748895,"user_tz":-60,"elapsed":9762,"user":{"displayName":"Ola Gołębiowska","userId":"06307342828668766183"}},"outputId":"a2af23d6-5eb4-476d-ccd8-92d7bcbd6987"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 134142 unique tokens.\n","Shape of data tensor: (11314, 1000)\n"]}]},{"cell_type":"code","source":["# here is a rule of thumb that says min(50, num_categories/2)\n","# If we’re in a hurry, one rule of thumb is to use the fourth root of the\n","# total number of unique categorical elements while another is that the embedding\n","# dimension should be approximately 1.6 times the square root of the number of\n","# unique elements in the category, and no less than 600.\n","EMBEDDING_DIM = 300\n","\n","# decay_steps = 1000\n","# initial_learning_rate = 0.1\n","# lr_scheduler = tf.keras.optimizers.schedules.CosineDecay(\n","#     initial_learning_rate, decay_steps)\n","\n","optimizer = tf.keras.optimizers.Adam(\n","    # learning_rate=0.0008,\n","    # weight_decay=0.0001,\n",")\n","# optimizer = tf.keras.optimizers.SGD()\n","network = models.Sequential()\n","network.add(Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n","network.add(Flatten())\n","network.add(layers.Dense(256, activation='relu'))\n","network.add(layers.Dense(20, activation='softmax')) # 20 because 20 classes\n","network.compile(optimizer=optimizer,#<--'SGD'\n","                loss='categorical_crossentropy',\n","                metrics=['accuracy'])\n"],"metadata":{"id":"KrvPfYnkSXvn","executionInfo":{"status":"ok","timestamp":1701513748895,"user_tz":-60,"elapsed":2,"user":{"displayName":"Ola Gołębiowska","userId":"06307342828668766183"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["history = network.fit(train_data,\n","                    train_target,\n","                    epochs=100,\n","                    batch_size=32,\n","                    validation_data=(test_data, test_target))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iak5UoGsSNsh","outputId":"74a5571b-16b7-466d-f04f-3280114277eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","354/354 [==============================] - 28s 79ms/step - loss: 2.5620 - accuracy: 0.2813 - val_loss: 1.3639 - val_accuracy: 0.5774\n","Epoch 2/100\n","354/354 [==============================] - 28s 78ms/step - loss: 0.3658 - accuracy: 0.9000 - val_loss: 1.0012 - val_accuracy: 0.7053\n","Epoch 3/100\n","133/354 [==========>...................] - ETA: 14s - loss: 0.0214 - accuracy: 0.9967"]}]},{"cell_type":"code","source":["# loss: 0.0026 - accuracy: 0.9994 - val_loss: 1.2574 - val_accuracy: 0.7442\n","# MAX_SEQUENCE_LENGTH = 1000\n","# MAX_WORDS = 20000\n","# EMBEDDING_DIM = 300\n","# optimizer = tf.keras.optimizers.Adam()\n","# network = models.Sequential()\n","# network.add(Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n","# network.add(Flatten())\n","# network.add(layers.Dense(256, activation='relu'))\n","# network.add(layers.Dense(20, activation='softmax')) # 20 because 20 classes\n","# network.compile(optimizer=optimizer,#<--'SGD'\n","#                 loss='sparse_categorical_crossentropy',\n","#                 metrics=['accuracy'])\n","# history = network.fit(train_data,\n","#                     train.target,\n","#                     epochs=200,\n","#                     batch_size=64,\n","#                     validation_data=(test_data, test.target))\n","\n","\n","\n","# oss: 0.0040 - accuracy: 0.9994 - val_loss: 0.9964 - val_accuracy: 0.7540\n","# categorical\n","# embed 50, max words 1000, max seq 300\n","# optimizer = tf.keras.optimizers.Adam()\n","# network = models.Sequential()\n","# network.add(Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n","# network.add(Flatten())\n","# network.add(layers.Dense(256, activation='relu'))\n","# network.add(layers.Dense(20, activation='softmax')) # 20 because 20 classes\n","# network.compile(optimizer=optimizer,#<--'SGD'\n","#                 loss='categorical_crossentropy',\n","#                 metrics=['accuracy'])\n","\n","\n","\n","# loss: 0.0038 - accuracy: 0.9992 - val_loss: 1.4800 - val_accuracy: 0.7002\n","#  jak wyżej\n","# network.add(layers.Dense(512, activation='relu'))\n","# network.add(layers.Dense(256, activation='relu'))\n","# network.add(layers.Dense(128, activation='relu'))\n","# network.add(layers.Dense(20, activation='softmax')) # 20 because 20 classe\n","\n","# jak wyżej\n","# emebedding 60 = 1.6 * sqrt\n","# loss: 1.2944e-05 - accuracy: 1.0000 - val_loss: 1.2793 - val_accuracy: 0.7102\n","# network = models.Sequential()\n","# network.add(Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n","# network.add(Flatten())\n","# network.add(layers.Dense(512, activation='relu'))\n","# network.add(layers.Dense(128, activation='relu'))\n","# network.add(layers.Dense(20, activation='softmax')) # 20 because 20 classes\n","# network.compile(optimizer=optimizer,#<--'SGD'\n","#                 loss='categorical_crossentropy',\n","#                 metrics=['accuracy'])\n","\n","\n","# loss: 1.6622e-04 - accuracy: 0.9999 - val_loss: 1.7396 - val_accuracy: 0.6747\n","# network = models.Sequential()\n","# network.add(Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n","# network.add(Flatten())\n","# network.add(layers.Dense(800, activation='relu'))\n","# network.add(layers.Dense(64, activation='relu'))\n","# network.add(layers.Dense(20, activation='softmax')) # 20 because 20 classes\n","# network.compile(optimizer=optimizer,#<--'SGD'\n","#                 loss='categorical_crossentropy',\n","#                 metrics=['accuracy'])\n","\n","#  loss: 4.0713e-08 - accuracy: 1.0000 - val_loss: 3.0106 - val_accuracy: 0.6508\n","# network = models.Sequential()\n","# network.add(Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n","# network.add(Flatten())\n","# network.add(layers.Dense(128, activation='relu'))\n","# network.add(layers.Dense(64, activation='relu'))\n","# network.add(layers.Dense(32, activation='relu'))\n","# network.add(layers.Dense(20, activation='softmax')) # 20 because 20 classes\n","# network.compile(optimizer=optimizer,#<--'SGD'\n","#                 loss='categorical_crossentropy',\n","#                 metrics=['accuracy'])\n","\n","\n","# loss: 7.6941e-07 - accuracy: 1.0000 - val_loss: 2.0702 - val_accuracy: 0.6827\n","# network = models.Sequential()\n","# network.add(Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n","# network.add(Flatten())\n","# network.add(layers.Dense(256, activation='relu'))\n","# network.add(layers.Dense(32, activation='relu'))\n","# network.add(layers.Dense(20, activation='softmax')) # 20 because 20 classes\n","# network.compile(optimizer=optimizer,#<--'SGD'\n","#                 loss='categorical_crossentropy',\n","#                 metrics=['accuracy'])\n","\n","\n","# loss: 1.3598e-07 - accuracy: 1.0000 - val_loss: 1.3201 - val_accuracy: 0.7228\n","# network = models.Sequential()\n","# network.add(Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n","# network.add(Flatten())\n","# network.add(layers.Dense(256, activation='relu'))\n","# network.add(layers.Dense(20, activation='softmax')) # 20 because 20 classes\n","# network.compile(optimizer=optimizer,#<--'SGD'\n","#                 loss='categorical_crossentropy',\n","#                 metrics=['accuracy'])\n","\n","\n","#  loss: 2.0386e-07 - accuracy: 1.0000 - val_loss: 1.3548 - val_accuracy: 0.6920\n","# MAX_SEQUENCE_LENGTH = 500\n","# MAX_WORDS = 10000\n","\n","# loss: 4.4888e-06 - accuracy: 1.0000 - val_loss: 1.1738 - val_accuracy: 0.7095\n","# network = models.Sequential()\n","# network.add(Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n","# network.add(Flatten())\n","# network.add(layers.Dense(513, activation='relu'))\n","# network.add(layers.Dense(20, activation='softmax')) # 20 because 20 classes\n","# network.compile(optimizer=optimizer,#<--'SGD'\n","#                 loss='categorical_crossentropy',\n","#                 metrics=['accuracy'])\n","\n","\n","# z 256 neuronów w jednej wartstwie\n","# loss: 7.9624e-08 - accuracy: 1.0000 - val_loss: 1.3645 - val_accuracy: 0.7189 <--- 1. epoka, później spadało\n","# history = network.fit(train_data,\n","#                     train_target,\n","#                     epochs=100,\n","#                     batch_size=16,\n","#                     validation_data=(test_data, test_target))\n","\n","\n","#  loss: 2.5353e-07 - accuracy: 1.0000 - val_loss: 1.2517 - val_accuracy: 0.7212\n","# history = network.fit(train_data,\n","#                     train_target,\n","#                     epochs=100,\n","#                     batch_size=32,\n","#                     validation_data=(test_data, test_target))\n","\n","\n","# loss: 1.3223e-07 - accuracy: 1.0000 - val_loss: 1.2755 - val_accuracy: 0.7209 <-- druga epoka, później spada\n","# history = network.fit(train_data,\n","#                     train_target,\n","#                     epochs=100,\n","#                     batch_size=128,\n","#                     validation_data=(test_data, test_target))"],"metadata":{"id":"pemNx54wfMpL","executionInfo":{"status":"aborted","timestamp":1701507198663,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ola Gołębiowska","userId":"06307342828668766183"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history_dict = history.history\n","import matplotlib.pyplot as plt\n","\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","ep=len(acc)\n","epochs = range(1, ep)\n","\n","# \"bo\" is for \"blue dot\"\n","plt.plot(epochs,acc[1:ep], 'bo', label='Training accuracy')\n","# b is for \"solid blue line\"\n","plt.plot(epochs, val_acc[1:ep], 'b', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"gMBHsyVr46mw","executionInfo":{"status":"aborted","timestamp":1701507198663,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ola Gołębiowska","userId":"06307342828668766183"}}},"execution_count":null,"outputs":[]}]}